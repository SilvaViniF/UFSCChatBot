# -*- coding: utf-8 -*-
"""Lama3LlamaIndexRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LgYtDgJlseOe78fauU8DXMawShL8YiQg
"""

"""pip install -U transformers

!pip install -q pypdf
!pip install -q python-dotenv
!pip install  llama-index==0.10.12
!pip install -q gradio
!pip install einops
!pip install accelerate

!pip install llama-index-llms-huggingface

!pip install llama-index-embeddings-fastembed

!pip install fastembed"""


import logging
import sys

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core import Settings


"mudar pdffolder pra pasta que tem seu PDF:"

pdffolder = "backend\pdffolder"


documents = SimpleDirectoryReader(pdffolder).load_data()

from llama_index.embeddings.fastembed import FastEmbedEmbedding

embed_model = FastEmbedEmbedding(model_name="BAAI/bge-small-en-v1.5")
Settings.embed_model = embed_model
Settings.chunk_size = 512

from llama_index.core import PromptTemplate


system_prompt = "You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided."


# This will wrap the default prompts that are internal to llama-index
query_wrapper_prompt = PromptTemplate("<|USER|>{query_str}<|ASSISTANT|>")

from huggingface_hub import HfApi
from getpass import getpass

from huggingface_hub import login

def authenticate():
    login()

# Call the authenticate function to log in
authenticate()
"usa essa key do hugging face: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"

import torch
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "meta-llama/Meta-Llama-3-8B-Instruct"

)

stopping_ids = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>"),
]

llm = HuggingFaceLLM(
    context_window=8192,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.7, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="meta-llama/Meta-Llama-3-8B-Instruct",
    model_name="meta-llama/Meta-Llama-3-8B-Instruct",
    device_map="auto",
    stopping_ids=stopping_ids,
    tokenizer_kwargs={"max_length": 4096},
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={"torch_dtype": torch.float16}
)

Settings.llm = llm
Settings.chunk_size = 512

index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine()

def predict(input, history):
  response = query_engine.query(input)
  return str(response)

import gradio as gr

gr.ChatInterface(predict).launch(share=True)

"acessar o ip que ele mostra localhost"